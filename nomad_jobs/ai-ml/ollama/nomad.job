job "ollama" {
  region = var.region
  datacenters = [var.datacenter]
  type        = "service"

  meta {
    job_file = "nomad_jobs/ai-ml/ollama/nomad.job"
    version = "4"
  }

  group "web" {
    network {
      mode = "host"
      port "web" {
        static = "11434"
        host_network = "lan"
      }
    }

    update {
      max_parallel     = 1
      min_healthy_time = "30s"
      auto_revert      = true
    }

    task "ollama" {
      driver = "docker"

      config {
        image = "ollama/ollama"
        runtime = "nvidia"
        dns_servers = [var.dns_server_ip]
        volumes = [
          "${var.ollama_data_dir}:/root/.ollama",
        ]
        ports = ["web"]
      }

      env {
        # Make the GPU visible to this container.
        NVIDIA_VISIBLE_DEVICES       = "all"
        NVIDIA_DRIVER_CAPABILITIES   = "compute,utility"
        # Pre-pull models on startup
        OLLAMA_MODELS               = "llama3.2:3b,codellama:7b"
      }

      service {
        name = "${NOMAD_JOB_NAME}"
        tags = ["traefik.enable=true"]
        port = "web"

        check {
          type     = "tcp"
          port     = "web"
          interval = "30s"
          timeout  = "2s"
        }
      }

      resources {
        cpu    = "200"
        memory = "7000"
      }
    }
  }
}

variable "region" {
    type = string
}

variable "shared_dir" {
    type = string
}

variable "ollama_data_dir" {
  type = string
}

variable "datacenter" {
  type = string
}

variable "dns_server_ip" {
  type = string
}
